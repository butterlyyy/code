{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of token\n",
    "#is it stop word,punctuation, digit,,\n",
    "#lower case, upper case, title case, bracket,quote, number, url,email\n",
    "#part of speech- Converting into df\n",
    "# drop punctuation\n",
    "# 1&2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724422d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#The line of code \"nlp=spacy.blank(\"en\")\" creates a new, blank spaCy language model for the English language.\n",
    "nlp=spacy.blank(\"en\")\n",
    "doc1=nlp('Today is Tuesday, 3rd Jan. We are starting NLP course !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932a285f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Today is Tuesday, 3rd Jan. We are starting NLP course"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(doc1)\n",
    "doc1[0]\n",
    "doc1[1]\n",
    "doc1[2:7]\n",
    "doc1[:9]\n",
    "doc1[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words- All\n",
    "print(STOP_WORDS)\n",
    "len(STOP_WORDS)\n",
    "# Stop words- All\n",
    "print(STOP_WORDS)\n",
    "len(STOP_WORDS)\n",
    "\n",
    "# Checking if tokens are stop words\n",
    "for token in doc1:\n",
    "    print(token, '===> ',token.is_stop)\n",
    "# To find the number of non-stop words in the tokens\n",
    "count=0\n",
    "for token in doc1:\n",
    "    if token.is_stop==False:\n",
    "        print(token)\n",
    "        count=count+1\n",
    "print('The no of non-stop words:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5928ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e7f73",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b688fa0",
   "metadata": {},
   "source": [
    "In summary, using spacy.load('en_core_web_sm') is a good option if you want to use a pre-trained model for general NLP tasks, while using spacy.blank(\"en\") is a good option if you want to create a custom NLP pipeline and train it from scratch for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169a5efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "India, a South Asian nation, is"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "type(nlp)\n",
    "doc1=nlp('India, a South Asian nation, is')\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c101a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "for token in doc1:\n",
    "    print(token)\n",
    "len(doc1)\n",
    "# Count of tokens\n",
    "t_count=0\n",
    "for token in doc1:\n",
    "    t_count=t_count+1\n",
    "    print(token)\n",
    "print('\\n\\n The total no of tokens:',t_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "Is it a stop ?\n",
    "# Count of non-stop words\n",
    "s_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_stop)\n",
    "print('\\n\\n The non-stop words:')\n",
    "for token in doc1:\n",
    "    if token.is_stop==False:\n",
    "        s_count=s_count+1\n",
    "        print(token)\n",
    "print('\\n The count of non-stop tokens:',s_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22aae9",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it a punctuation ?\n",
    "# Count of non-punctuation tokens\n",
    "\n",
    "p_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_punct)\n",
    "print('\\n\\n The non-punctutation tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_punct==False:\n",
    "        p_count=p_count+1\n",
    "        print(token)\n",
    "print('\\n The count of non-punctuation tokens:',p_count)\n",
    "\n",
    "# Count of punctuation tokens\n",
    "\n",
    "p_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_punct)\n",
    "print('\\n\\n The non-punctutation tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_punct==True:\n",
    "        p_count=p_count+1\n",
    "        print(token)\n",
    "print('\\n The count of non-punctuation tokens:',p_count)\n",
    "\n",
    "# Count of left-punctuation tokens\n",
    "\n",
    "lp_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_left_punct)\n",
    "print('\\n\\n The left punctutation tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_left_punct==True:\n",
    "        lp_count=lp_count+1\n",
    "        print(token)\n",
    "print('\\n The count of left -punctuation tokens:',lp_count)\n",
    "\n",
    "# Count of right punctuation tokens\n",
    "\n",
    "rp_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_right_punct)\n",
    "print('\\n\\n The right punctutation tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_right_punct==True:\n",
    "        rp_count=rp_count+1\n",
    "        print(token)\n",
    "print('\\n The count of right -punctuation tokens:',rp_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e873fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#is it an alphabet?\n",
    "# Count of alphabets - tokens\n",
    "a_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_alpha)\n",
    "print('\\n\\n The alphabet-tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_alpha==True:\n",
    "        a_count=a_count+1\n",
    "        print(token)\n",
    "print('\\n The count of alphabet-tokens:',a_count)\n",
    "\n",
    "# Count of digit tokens\n",
    "d_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_digit)\n",
    "print('\\n\\n The digit tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_digit==True:\n",
    "        d_count=d_count+1\n",
    "        print(token)\n",
    "print('\\n The count of digit tokens:',d_count)\n",
    "\n",
    "#Is it lower case?\n",
    "l_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_lower)\n",
    "print('\\n\\n The lower case tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_lower==True:\n",
    "        l_count=l_count+1\n",
    "        print(token)\n",
    "print('\\n The count of lower case tokens:',l_count)\n",
    "\n",
    "#Is it upper case?\n",
    "# Count of upper case tokens\n",
    "\n",
    "u_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_upper)\n",
    "print('\\n\\n The upper case tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_upper==True:\n",
    "        u_count=u_count+1\n",
    "        print(token)\n",
    "print('\\n The count of upper case tokens:',u_count)\n",
    "\n",
    "#Is it title case?\n",
    "# Count of title case tokens\n",
    "\n",
    "t_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_title)\n",
    "print('\\n\\n The title case tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_title==True:\n",
    "        t_count=t_count+1\n",
    "        print(token)\n",
    "print('\\n The count of title case tokens:',t_count)\n",
    "\n",
    "#is it a bracket?\n",
    "# Count of bracket tokens\n",
    "b_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_bracket)\n",
    "print('\\n\\n The bracket tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_bracket==True:\n",
    "        b_count=b_count+1\n",
    "        print(token)\n",
    "print('\\n The count of bracket tokens:',b_count)\n",
    "\n",
    "# is it a quote\n",
    "# Count of quote tokens\n",
    "q_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.is_quote)\n",
    "print('\\n\\n The quote tokens:')\n",
    "for token in doc1:\n",
    "    if token.is_quote==True:\n",
    "        q_count=q_count+1\n",
    "        print(token)\n",
    "print('\\n The count of quote tokens:',q_count)\n",
    "\n",
    "#is it like a number?\n",
    "# Count of number tokens\n",
    "n_count=0\n",
    "for token in doc1:\n",
    "    print(token,'==>',token.like_num)\n",
    "print('\\n\\n The num tokens:')\n",
    "for token in doc1:\n",
    "    if token.like_num==True:\n",
    "        n_count=n_count+1\n",
    "        print(token)\n",
    "print('\\n The count of number tokens:',n_count)\n",
    "\n",
    "#is it like a url?\n",
    "for token in doc4:\n",
    "    print(token, '==>', token.like_url)\n",
    "    \n",
    "#Is it like an email ID?\n",
    "for token in doc5:\n",
    "    print(token.text,'==>',token.like_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b334667",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=nlp('100000 is a big number')\n",
    "for token in doc2:\n",
    "    print(token,'==>',token.is_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375941c",
   "metadata": {},
   "source": [
    "### Parts of Speech - POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token, '==>', token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c995db1",
   "metadata": {},
   "source": [
    "###  Converting into a Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('AUX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd601d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Token','POS', 'Explain_POS','TAG','Explain_TAG']\n",
    "cols\n",
    "rows=[]\n",
    "for token in doc1:\n",
    "    row=token,token.pos_,spacy.explain(token.pos_),token.tag_,spacy.explain(token.tag_)\n",
    "    rows.append(row)\n",
    "rows\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "token_df=pd.DataFrame(rows,columns=cols)\n",
    "token_df\n",
    "\n",
    "# Count of each POS\n",
    "\n",
    "token_df['POS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41579380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to drop punctuation\n",
    "text = \"This is a sentence! It contains some punctuation marks.\"\n",
    "\n",
    "# Import the necessary libraries and load the language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Use a list comprehension to create a list of non-punctuation tokens\n",
    "tokens = [token for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the tokens back into a string\n",
    "clean_text = \" \".join([token.text for token in tokens])\n",
    "\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and load the language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text to be processed\n",
    "text = \"This is a sentence! It contains some punctuation marks.\"\n",
    "\n",
    "# Process the text with the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Use the .drop() function to remove the punctuation tokens\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        token.drop()\n",
    "\n",
    "# Print the remaining tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510791ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
