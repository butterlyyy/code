{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961535c0",
   "metadata": {},
   "source": [
    "1)\tHDFS Architecture\n",
    "\n",
    "2)\tExplain HDFS Read\n",
    "\n",
    "3)\tHDFS Write\n",
    "\n",
    "4)\tMRv1 Process \n",
    "\n",
    "5)\tMRv2 Process \n",
    "\n",
    "6)\tDifference in failure recovery of MRV1 and MRv2\n",
    "\n",
    "7)\tMap Reduce Failure Recovery\n",
    "\n",
    "8)\tApache Hive Architecture\n",
    "\n",
    "9)\tApache Spark and Apache Hadoop\n",
    "\n",
    "10)\tHadoop Application\n",
    "\n",
    "11)\tTo Whom Does Big Data Matters Most, HDFS – Add Node To Cluster,HDFS – Remove Node From Cluster\n",
    "\n",
    "12)\tApache Spark Architecture\n",
    "\n",
    "13)\tApache Hive Architecture\n",
    "\n",
    "14)\tHive – Features \n",
    "\n",
    "15)\tColumnar Database \n",
    "\n",
    "16)\t.5 Vs \n",
    "\n",
    "\n",
    "#1.HDFS Architecture\n",
    "Hadoop Distributed File System (HDFS) is an open-source distributed file system designed to store and process large datasets across clusters of commodity hardware. HDFS is not designed for small files, and modifications cannot be made at random offsets of files, resulting in low-latency data access.\n",
    "Individual files are broken into blocks of fixed size, typically 256 MB, and stored across a cluster of nodes. The HDFS cluster can be accessed concurrently by multiple clients, and metadata information is handled by a single machine, the Name Node. The Name Node knows the status and metadata of all the files in HDFS, including file names, permissions, and locations of each block of the file. Data is stored on the Data Node, which reads data and sends it to the client when retrieval is requested.\n",
    "The HDFS architecture consists of two main components:\n",
    "The Name Node function is critical for the overall health of HDFS. If the Name Node crashes, all information and the complete file system can be lost irrecoverably. Therefore, metadata involvement of the Name Node in data transfer is kept minimal, and the Name Node can work in high availability mode.\n",
    "Name Node▪ Name Node is controller and manager of HDFS▪ It knows the status and the metadata of all the files in HDFS▪ Metadata [ file names, permissions, and locations of each block of file ]▪ HDFS cluster can be accessed concurrently by multiple clients, even then this metadata information is never desynchronized; hence, all this information is handled by a single machine▪ Since metadata is typically small, all this info is stored in main memory of Name Node, allowing fast access to metadata\n",
    "\n",
    "A Secondary Name Node is a helper of the Name Node and performs periodic checkpoints, communicating with the Name Node to take snapshots of HDFS metadata. These snapshots help minimize downtime and loss of data.\\\n",
    "Secondary Name Node\n",
    "▪ It is not backup of name node nor data nodes connect to this; it is just a helper of name node.\n",
    "▪ It only performs periodic checkpoints\n",
    "▪ It communicates with name node and to take snapshots of HDFS metadata\n",
    "▪ These snapshots help minimize downtime and loss of data\n",
    "\n",
    "#2. Explain HDFS Read\n",
    "HDFS is a distributed file system designed to store and manage large data sets across multiple computers. It allows for reliable storage and efficient retrieval of data, even when the data set exceeds the storage capacity of a single computer.A HDFS read is the process of retrieving data from the HDFS and returning it to the client. The HDFS read process includes the following steps:\n",
    "1.\tThe client sends a read request to the NameNode:\n",
    "•\tThe client application sends a read request for a specific file to the NameNode, which is the central node in the HDFS architecture that maintains the metadata of the file system.\n",
    "•\tThe metadata includes information about the file such as its location, the size of the file, and the number of data blocks it's divided into.\n",
    "2.\tNameNode returns the location of the data blocks to the client:\n",
    "•\tUpon receiving the read request, the NameNode searches its metadata to determine the locations of the data blocks that contain the requested file data.\n",
    "•\tThe NameNode then sends this information to the client, which includes the IP addresses of the DataNodes that hold the data blocks.\n",
    "3.\tClient contacts the DataNode that holds the data block and requests the data:\n",
    "•\tThe client contacts one of the DataNodes based on the information provided by the NameNode to request the specific data block that contains the requested file data.\n",
    "•\tIf the first DataNode is not available or the response is slow, the client can request the data from one of the other replicas of the same data block.\n",
    "4.\tDataNode reads the data and returns it to the client:\n",
    "•\tOnce the client establishes communication with the DataNode that holds the requested data block, the DataNode reads the data from its local storage and sends it back to the client.\n",
    "5.\tClient receives the data and can process it as needed:\n",
    "•\tUpon receiving the requested data block from the DataNode, the client application can process the data as needed for the desired analysis or computation.\n",
    "•\tIf the file is stored across multiple data blocks, the client repeats the process of contacting the corresponding DataNodes and requesting the data block until it retrieves all the data blocks that contain the entire file.\n",
    "\n",
    "HDFS reads are optimized for large sequential access patterns and are not suitable for random or small reads. However, the HDFS architecture provides the ability to replicate data blocks across multiple nodes, ensuring high availability and fault tolerance.\n",
    "#3. HDFS Write\n",
    "HDFS write refers to the process of storing data in the Hadoop Distributed File System. When data is written to HDFS, it is divided into blocks and stored across multiple nodes in a cluster. This helps to distribute the data and increase its fault tolerance.\n",
    "Here's a basic overview of the HDFS write process:\n",
    "1.\tClient sends data: A client application sends data to be stored in HDFS to the NameNode, which is the master node responsible for managing the metadata of the file system.\n",
    "2.\tNameNode decides block placement: The NameNode decides where to place the blocks of data by determining which DataNodes have available space and are closest to the client.\n",
    "3.\tDataNode stores the block: The NameNode sends the data to the designated DataNodes, which store the data on local disk. The data is stored as multiple replicas, usually three, to ensure data availability and fault tolerance.\n",
    "4.\tAcknowledgment to client: Once the data is stored on the DataNodes, the NameNode sends an acknowledgment to the client, indicating that the write operation was successful.\n",
    "In HDFS, writes are sequential, meaning that new data is always appended to the end of a file. This helps to optimize the write process and ensure consistency and data integrity. However, this also means that modifying existing data in HDFS is not an efficient process, as it requires rewriting the entire file.\n",
    "What Is Map-Reduce \n",
    "▪ Split input files (e.g., by HDFS blocks) ▪ Move code to data ▪ Operate on key / value pairs ▪ Mappers filter and transform input data ▪ Sort & Shuffle provides order to mapper data ▪ Reducers aggregate the mapper output (post Sort & Shuffle) ▪ Stores Reducer output in HDFS\n",
    "Map-Reduce Steps\n",
    "▪ Split input data in independent chunks is already available via HDFS ▪ Job needs to be scheduled to carry out required process ▪ Schedule tasks on nodes where data is already present ▪ Map Phase – Transformation Phase => input Data | output – list pairs ▪ Sort & Shuffle – Group & Order Phase => input – list of pairs | output – sorted & grouped list of pairs ▪ Reduce Phase – Aggregation Phase => sorted & grouped list of pairs | output – aggregated Observations ▪ Manager required for scheduling jobs, collating results & updating status ▪ Process and storage nodes are same. i.e., MR-Tasks and HDFS-DataNode run on same machine.\n",
    "\n",
    "\n",
    "#4.Explain map reduce job flow in mrv1\n",
    " MRv1 Process ▪ A Client invokes a Map-Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster) ▪ An instance of Job Tracker is created in the memory of the Calling Node. ▪ The Job Tracker queries the Name Node and finds the Data Nodes (location of the data to be used). ▪ Job Tracker then launches Task Trackers in the memory of all the Data Nodes as above to run the jobs ▪ Job Tracker gives the code to Task Tracker to run as a Task ▪ Task Tracker is responsible for creating the tasks & running the tasks ▪ In effect the Mapper of the Job is found here ▪ Once the Task is completed, the result from the Tasks is sent back to the Job Tracker ▪ Job Tracker also keeps a track of progress by each Task Tracker ▪ The Job Tracker also receives the results from each Task Tracker and aggregates the results ▪ In effect the Reducer of the Job is found here\n",
    "In MRv1 (MapReduce Version 1), the flow of a MapReduce job can be explained as follows:\n",
    "1.\tInput data is divided into smaller chunks and stored across multiple nodes in the Hadoop cluster.\n",
    "2.\tThe client node submits a MapReduce job to the JobTracker, which is the master node in MRv1.\n",
    "3.\tThe JobTracker divides the job into a number of smaller map and reduce tasks and assigns them to TaskTrackers, which are the slave nodes in the cluster.\n",
    "4.\tThe map tasks process the input data chunks and produce intermediate results. The intermediate results are grouped by key and sent to the reduce tasks.\n",
    "5.\tThe reduce tasks aggregate the intermediate results to produce the final output, which is stored back in the Hadoop cluster.\n",
    "6.\tThe client node receives the final output and can use it as desired.\n",
    "This entire process is managed by MRv1, which takes care of the distribution of data and tasks, error handling, and coordination between nodes in the cluster. The main idea behind MapReduce is to parallelize the processing of large amounts of data, making it possible to handle big data problems with a cluster of commodity hardware.\n",
    "Map-Reduce Failure Recovery MRv1 ▪ Task Failure – new task is started by the Task Tracker ▪ Task Tracker Failure – new Task Tracker is started by the Job Tracker ▪ Job Tracker Failure – no recovery; single point of failure\n",
    "\n",
    "#5.Explain map reduce job flow in mrv2.\n",
    "        ▪ A Client invokes a Map-Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster) ▪ An instance of Resource Manager is created in the memory of the Calling Node. ▪ The Resource Manager then launches containers with appropriate resources (memory) with App Node Manager in memory of the Calling Node ▪ Along with this Application Master is invoked. Application Master is “pause” mode till all containers with Task Node Manager (as below) are created. ▪ The Resource Manager queries the Name Node and finds the Data Nodes (location of the data used). ▪ The Resource Manager then launches containers with appropriate resources (memory) with Task Node Manager in all the Data Nodes as above to run the jobs ▪ Application Master gives the code to Task Node Manager to run as a Task ▪ Task Node Manager is responsible for creating & running tasks. In effect the Mapper of the Job is here ▪ Once the Task is completed, the result from the Tasks is sent back to the Application Master ▪ Application Master also keeps a track of progress by each Task Node Manager ▪ The Application Master lso receives the results from each Task Node Manager and aggregates the results ▪ In effect the Reducer of the Job is found here ▪ Thus from previous version, Job Tracker has been replaced by Resource Manager & Application Master ▪ From previous version, TaskTracker has been replaced by Task Node Managers \n",
    "In MRv2 (MapReduce Version 2), the flow of a MapReduce job can be explained as follows:\n",
    "1.\tInput data is divided into smaller chunks and stored across multiple nodes in the Hadoop cluster.\n",
    "2.\tThe client node submits a MapReduce job to the Resource Manager, which is the master node in MRv2 and part of the YARN (Yet Another Resource Negotiator) framework.\n",
    "3.\tThe Resource Manager allocates resources, such as memory and CPU, to the MapReduce job and launches ApplicationMaster, which is responsible for managing the job.\n",
    "4.\tThe ApplicationMaster divides the job into a number of smaller map and reduce tasks and assigns them to NodeManager, which are the slave nodes in the cluster.\n",
    "5.\tThe map tasks process the input data chunks and produce intermediate results. The intermediate results are grouped by key and sent to the reduce tasks.\n",
    "6.\tThe reduce tasks aggregate the intermediate results to produce the final output, which is stored back in the Hadoop cluster.\n",
    "7.\tThe Resource Manager receives the final output and sends it to the client node, which can use it as desired.\n",
    "MRv2 provides several improvements over MRv1, including better scalability, the ability to process data in memory, and improved support for running multiple applications on the same cluster. With MRv2 and YARN, it is possible to run multiple types of data processing applications, such as batch processing, interactive SQL, and machine learning, on the same Hadoop cluster, providing a more flexible and versatile platform for big data processing.\n",
    " Failure recovery of MRV2\n",
    "1.\tIf a Task fails, the Task Node Manager starts a new task.\n",
    "2.\tIf the Task Node Manager fails, the Resource Manager creates a new container with a Task Node Manager, and the Application Master sends  the code.\n",
    "3.\tIf the Application Master fails, the App Node Manager starts a new Application Master.\n",
    "If the App Node Manager fails, the Resource Manager creates a new container with the App Node Manager and it invokes the Application Master.\n",
    "4.\tIn case of Resource Manager failure, a new Resource Manager is started with the saved state.\n",
    "These steps ensure that the system continues to operate even in case of component failures and prevents data loss.\n",
    "Map-Reduce Features ▪ Simple interface ▪ Parallelization and distribution of tasks ▪ Automatic partition ▪ Sorting ▪ Merging ▪ Fault-tolerance ▪ Status and monitoring\n",
    "#6.Difference in failure recovery of MRV1 and MRv2\n",
    "In MRv1, when a slave node fails, the master node is responsible for detecting the failure and reassigning the tasks that were assigned to the failed node to other nodes. The intermediate results produced by the map function are stored on disk, so they can be retrieved and processed even if a node fails.\n",
    "In MRv2, the YARN component provides enhanced failure recovery capabilities by managing resources and tasks in a more efficient way. If a node fails, YARN can detect the failure and automatically reassign the tasks that were assigned to the failed node to other nodes. This helps to ensure that the processing continues even in the event of a node failure.\n",
    "Overall, MRv2 provides improved failure recovery compared to MRv1, as it allows for more efficient management of resources and tasks and provides enhanced monitoring capabilities to detect and recover from node failures.\n",
    "#7.Map Reduce Failure Recovery\n",
    "MapReduce is a programming model and software framework for processing large-scale data in parallel across a distributed cluster of computers. When running a MapReduce job, there is always a risk of failure, which can occur due to various reasons such as hardware failure, software bugs, network issues, or even power outages. To ensure the successful completion of a MapReduce job, the framework provides mechanisms for failure recovery, which are as follows:\n",
    "1.\tTask Retry: If a Map or Reduce task fails, the framework can retry the task on another node in the cluster. The task will be re-executed until it succeeds or reaches a configurable maximum number of attempts.\n",
    "2.\tSpeculative Execution: In some cases, a task may take longer than expected to complete due to slow processing or network congestion. To overcome this, the framework can run multiple instances of the same task on different nodes in the cluster. The first task to complete successfully will provide the output, and the others will be terminated.\n",
    "3.\tTask Redundancy: In addition to speculative execution, the framework can also provide redundancy by running multiple instances of the same task on different nodes simultaneously. This ensures that even if one instance fails, the job can still complete successfully.\n",
    "4.\tJob Restart: If the failure is catastrophic and the job cannot be recovered using the above mechanisms, the framework provides the option to restart the entire job from the beginning. This can be time-consuming, but it ensures that the job completes successfully.\n",
    "5.\tBy using these mechanisms, the MapReduce framework provides fault tolerance and ensures the successful completion of large-scale data processing jobs even in the face of failures.\n",
    "#8.Apache Hive Architecture\n",
    "Apache Hive is a data warehouse infrastructure built on top of Hadoop for querying and analyzing large datasets stored in Hadoop Distributed File System (HDFS). The architecture of Apache Hive includes:\n",
    "•\tMetastore: It is a centralized repository that stores the metadata of the Hive tables, partitions, columns, and other database objects. The metadata is stored in a database or a file system, and it is used by the driver and execution engine during query processing.\n",
    "•\tDriver: The driver is responsible for managing the lifecycle of a HiveQL statement. It receives the HiveQL queries from the user, analyzes the syntax, and builds an execution plan for the query. The driver also interacts with the metastore to retrieve the metadata of the tables involved in the query.\n",
    "•\tQuery Compiler: The query compiler receives the execution plan from the driver and compiles the HiveQL statements into a set of Map-Reduce tasks. The query compiler also optimizes the query plan by pushing down filters and joins, reducing the number of data scans required to complete the query.\n",
    "•\tExecution Engine: The execution engine runs the Map-Reduce tasks generated by the query compiler. It manages the dependencies between the tasks and ensures that they are executed in the correct order. The execution engine also monitors the progress of the tasks and reports the results back to the driver.\n",
    "•\tSystem: The system provides a user interface for viewing the results of the queries. It includes the Hive Client, which is a command-line interface that allows users to interact with Hive, and a web-based user interface that provides a graphical interface for managing Hive queries and jobs.\n",
    "Overall, the architecture of Apache Hive is designed to provide a user-friendly interface for querying and analyzing large datasets stored in HDFS. It leverages the power of Hadoop and Map-Reduce to process data in a distributed and parallel manner, allowing users to quickly extract insights from large datasets.\n",
    "#9.Apache Spark and Apache Hadoop\n",
    "Apache Spark and Apache Hadoop are both open-source big data processing frameworks used to store and process large datasets in distributed computing environments. However, there are several key differences and similarities between the two:\n",
    "Differences:\n",
    "1.\tProcessing Model: Hadoop uses a MapReduce processing model, which is efficient for batch processing, while Spark uses a more flexible DAG (Directed Acyclic Graph) processing model, which is optimized for in-memory processing of data.\n",
    "2.\tSpeed: Spark is generally faster than Hadoop because it keeps data in memory and processes it in parallel. Hadoop, on the other hand, reads and writes data to disk between processing stages.\n",
    "3.\tEase of Use: Spark provides a more user-friendly and expressive API, making it easier for developers to write and execute complex data processing tasks.\n",
    "4.\tIntegration: Hadoop is better integrated with other Hadoop ecosystem components such as Hive, Pig, and HBase. Spark, on the other hand, can integrate with these components but also has its own ecosystem of components like Spark SQL, Spark Streaming, and MLlib.\n",
    "5.\tData Processing: Hadoop is optimized for batch processing of large data sets, while Spark can handle batch processing as well as streaming, machine learning, and graph processing.\n",
    "Similarities:\n",
    "1.\tDistributed Processing: Both Spark and Hadoop are designed to process large datasets across a cluster of computers in a distributed manner.\n",
    "2.\tScalability: Both frameworks are highly scalable and can handle large datasets and a large number of nodes.\n",
    "3.\tOpen Source: Both Spark and Hadoop are open-source projects, which means they are freely available for anyone to use, modify, and distribute.\n",
    "4.\tData Storage: Both frameworks can store and process data in distributed file systems such as HDFS, S3, and others.\n",
    "In summary, Spark and Hadoop are both powerful big data processing frameworks with different strengths and use cases. While Hadoop is ideal for batch processing of large data sets, Spark is optimized for in-memory processing and can handle a wider range of tasks, including streaming, machine learning, and graph processing.\n",
    "\n",
    "#10.Hadoop Application\n",
    "Predictive Analysis ▪ Sentiment Analysis ▪ Customer Intelligence ▪ Fraud & Security Intelligence ▪ High-Performance Analytics ▪ Risk Management ▪ Operational Analysis\n",
    "\n",
    "#11.To Whom Does Big Data Matters Most\n",
    "▪ Any Organization / Entity That Has Webserver Or An Information Logging Utility ▪ Utilize It's Logs To Find Out Information Relevant For It's Business ▪ Get Insights Into Data To Increase Revenue And / Or Decrease Costs ▪ Functions • Marketing • Operations • Research • Finance ▪ On Sectors Such As • Healthcare • Customer Service • Insurance • Banking • Social Media • IoT\n",
    "HDFS – Add Node To Cluster\n",
    "▪ Node Added To The Cluster Through Configuration Changes ▪ Name Node Realizes One Extra Data Node Is Available ▪ Checks How Free Space Available ▪ Starts Sending Data To Write ▪ Then Operations As Normal\n",
    "\n",
    "HDFS – Remove Node From Cluster\n",
    "▪ Node Removed From The Cluster Through Configuration Changes ▪ Name Node Realizes One Data Node Is No Longer Available ▪ Checks How Much Data Was Stored On That Node ▪ Finds Out Where The Copy Of The Data Is Available On The Cluster ▪ Creates One More Copy Of That Data On The Cluster ▪ Once Done The Operations As Normal\n",
    "\n",
    "#12.Apache Spark Architecture\n",
    "Apache Spark is a distributed computing framework designed to process large datasets in a parallel and fault-tolerant manner. It consists of several components that work together to execute data processing jobs:\n",
    "1.\tDriver Program: The driver program is the main entry point for a Spark application. It runs the user code and creates a SparkContext, which is used to connect to a cluster manager (such as YARN or Mesos) and coordinate the execution of tasks across the cluster.\n",
    "2.\tCluster Manager: The cluster manager is responsible for managing resources in the cluster, including allocating and scheduling tasks to worker nodes. Spark can work with several cluster managers, including YARN, Mesos, and Kubernetes.\n",
    "3.\tExecutors: Executors are worker nodes that execute tasks assigned to them by the driver program. Each executor runs in a separate JVM and is responsible for managing a subset of the data and performing computations on that data.\n",
    "4.\tSparkContext: SparkContext is the entry point for interacting with Spark functionality from a programming language such as Scala, Python, or Java. It provides access to Spark's core functionality and coordinates the execution of tasks across the cluster.\n",
    "5.\tRDD: Resilient Distributed Datasets (RDDs) are the fundamental data structure in Spark. RDDs are immutable and distributed collections of objects that can be processed in parallel across a cluster. They can be created from data stored in Hadoop Distributed File System (HDFS), local file systems, or other data sources.\n",
    "6.\tDAG Scheduler: The DAG (Directed Acyclic Graph) scheduler is responsible for scheduling tasks across the cluster in a way that maximizes parallelism and minimizes data shuffling. It creates a DAG of stages based on the dependencies between tasks and schedules them for execution.\n",
    "7.\tTask Scheduler: The task scheduler is responsible for assigning tasks to individual executors for execution. It takes into account the data locality, i.e., it tries to assign tasks to nodes that have the data already in memory.\n",
    "8.\tCluster Manager: The cluster manager is responsible for managing resources in the cluster, including allocating and scheduling tasks to worker nodes. Spark can work with several cluster managers, including YARN, Mesos, and Kubernetes.\n",
    "In summary, Spark's architecture is based on a master-worker model where the driver program acts as the master and the executors run as workers. The SparkContext provides a unified entry point to interact with Spark's functionality, while RDDs provide an immutable and distributed data structure for processing large datasets. The DAG scheduler and task scheduler work together to schedule and execute tasks across the cluster in an optimized manner.\n",
    "#13.Apache Hive Architecture\n",
    "Apache Hive is a data warehousing solution that provides an SQL-like interface to query data stored in Hadoop Distributed File System (HDFS) or other compatible distributed storage systems. Hive architecture includes the following components:\n",
    "1.\tMetastore: The Metastore is a central repository that stores metadata about tables, partitions, columns, and other objects in Hive. It also stores schema information, such as data types, and statistics about the data stored in HDFS.\n",
    "2.\tHive Query Language (HQL): Hive provides an SQL-like language called Hive Query Language (HQL), which can be used to define tables, query data, and perform other operations on the data stored in HDFS.\n",
    "3.\tDriver: The Driver is the main component that receives queries submitted by users through HQL and executes them. It is responsible for parsing the query, optimizing the execution plan, and coordinating the execution of tasks across the cluster.\n",
    "4.\tExecution Engine: The Execution Engine is responsible for executing the tasks generated by the Driver. Hive supports multiple execution engines, including MapReduce, Tez, and Spark.\n",
    "5.\tSerDe: A Serializer-Deserializer (SerDe) is a library that converts data between Hive's internal representation and external formats such as CSV, JSON, or Avro.\n",
    "6.\tStorage Handler: A Storage Handler is a pluggable component that defines how data is stored and accessed in Hive. Hive supports several storage handlers, including Hadoop Distributed File System (HDFS), Apache HBase, and Apache Cassandra.\n",
    "7.\tPartition: A Partition is a subset of data in a table that is divided based on a specific column. Partitions allow for faster querying of large datasets by enabling the selective retrieval of only relevant data.\n",
    "In summary, Hive architecture is based on a layered approach where the Metastore stores metadata about the data stored in HDFS, while HQL provides an SQL-like interface for querying and manipulating the data. The Driver receives the queries submitted by users and generates a series of tasks that are executed by the Execution Engine. SerDes and Storage Handlers are pluggable components that define how data is serialized, deserialized, and stored in Hive. Partitions allow for faster querying of large datasets by dividing the data based on a specific column.\n",
    "#14.Hive – Features ▪ Data Definition Language ▪ Data Manipulation Language ▪ SQL like Hive Queries ▪ Pluggable input-output format ▪ Extendible • User Defined Functions – UDFs • User Defined Aggregate Functions – UDAF • User Defined Table Functions – UDTF\n",
    "\n",
    "#15.Columnar Database ▪ A columnar database, also known as a column-oriented database ▪ A DBMS that stores data in columns rather than in rows as RDBMS ▪ The table schema defines only column families, which are the key value pairs. ▪ A table can have multiple column families and each column family can have any number of columns. ▪ Subsequent column values are stored contiguously on the disk. ▪ Each cell value of the table has a timestamp. ▪ The tables in Columnar database are sorted by row. ▪ The rows of tables in Columnar database are identified by a row-key.\n",
    "Columnar Database ▪ The difference centered around performance, storage and schema modifying techniques. ▪ Efficient write & read data to and from hard disk storage; speeds up the time it takes to return a query. ▪ Main benefits columnar operations like MIN, MAX, SUM, COUNT and AVG performed very rapidly. ▪ Columnar Database allows random read & write in Hadoop. ▪ Columnar database fulfills the ACID principles of a database.\n",
    "A columnar database, also known as a column-oriented database, is a database management system that stores data in columns rather than in rows as relational database management systems (RDBMS). In a columnar database, each column is stored separately on disk, allowing for faster data retrieval and better compression rates.\n",
    "The table schema in a columnar database defines only column families, which are key-value pairs. A table can have multiple column families, and each column family can have any number of columns. Subsequent column values are stored contiguously on the disk, which enables efficient querying and aggregation operations.\n",
    "Each cell value of the table in a columnar database has a timestamp, allowing for tracking the changes to data over time. The tables in a columnar database are sorted by row, making it easy to retrieve a range of rows based on their row key.\n",
    "The rows of tables in a columnar database are identified by a row-key, which can be used to access a particular row or range of rows efficiently. The row-key can be any data type, and the columnar database typically provides efficient indexing mechanisms for the row-key.\n",
    "Columnar databases are widely used in big data and data warehousing applications, where large volumes of data need to be queried and analyzed efficiently. They are well-suited for analytical workloads that involve complex queries and require high performance and scalability.\n",
    "\n",
    "\n",
    "#16.5 Vs \n",
    "Big Data refers to large volumes of data that cannot be processed or analyzed by traditional data processing systems. The term is often used to describe data sets that are characterized by the following five Vs: volume, variety, velocity, veracity, and value.\n",
    "\n",
    "Volume:\n",
    "Volume refers to the scale of data, which is typically massive. Data volumes in the terabyte, petabyte, or even exabyte range are common in Big Data scenarios. For example, a large e-commerce company like Amazon generates an enormous amount of data from its website logs, customer transactions, and product reviews. Such a company can generate a massive amount of data in just a few hours, which can be challenging to store and analyze.\n",
    "\n",
    "Variety:\n",
    "Big Data is not just about the amount of data; it also includes data in different formats, including structured, semi-structured, and unstructured data. Structured data is organized and easily searchable, like data in a database, whereas unstructured data includes text, images, and videos, and can be challenging to analyze. Semi-structured data, like log files, has some structure but is not organized like a database. For example, a healthcare provider may have data from multiple sources, including electronic medical records, lab reports, and medical images, each in different formats.\n",
    "\n",
    "Velocity:\n",
    "The velocity of data refers to the speed at which data is generated, captured, and analyzed. The rise of the Internet of Things (IoT) and the proliferation of sensors and devices that collect data have accelerated the velocity at which data is generated. For example, social media platforms like Twitter generate an enormous amount of data in real-time, with millions of tweets sent every second.\n",
    "\n",
    "Veracity:\n",
    "Veracity refers to the accuracy and reliability of data. With Big Data, the quality of data can vary significantly. Data sources can be unreliable or incomplete, leading to inaccuracies in analysis. For example, data collected from social media may include fake or biased information, making it difficult to trust and use for decision-making.\n",
    "\n",
    "Value:\n",
    "The value of Big Data refers to the insights and actionable information that can be extracted from analyzing large and complex data sets. For example, analyzing customer data can help companies personalize marketing campaigns and improve customer satisfaction, leading to increased sales and revenue.\n",
    "\n",
    "In summary, the five Vs of Big Data (volume, variety, velocity, veracity, and value) describe the characteristics of large and complex data sets. Understanding these characteristics can help organizations design and implement effective Big Data solutions that enable them to unlock insights and value from their data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecd533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
